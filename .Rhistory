err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
n = nrow(X)
err = c()
for (i in 1:10){
learn = sample(1:n, 2/3*n)
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[-learn,])
# Compute the classification error
err[i] = sum(Y[-learn] != ystar$class) / length(ystar$class)
}
boxplot(err)
library(MASS)
V = 25
n = nrow(X)
fold = rep(1:V, n/V)
err.knn = err.lda = rep(NA, V)
for (v in 1:V){
learn = which(fold != v)
test = which(fold == v)
# KNN (with an internal CV for picking K)
X2 = X[learn,]; Y2 = Y[learn]
K = 30; n2 = nrow(X2)
fold2 = rep(1:V, n2/V)
err.knn2 = matrix(NA, K, V)
for (v2 in 1:v){ # Start of the internal CV
learn2 = which(fold2 != v2)
test2 = which(fold2 == v2)
for (k in 1:K){
ystar = knn(X2[learn2,], X2[test2,], Y2[learn2], k)
err.knn2[k, v2] = sum(ystar != Y2[test2]) / length(ystar)
}
}
Kstar = which.min(rowMeans(err.knn2)) 3 Choice of K* for this fold
library(MASS)
V = 25
n = nrow(X)
fold = rep(1:V, n/V)
err.knn = err.lda = rep(NA, V)
for (v in 1:V){
learn = which(fold != v)
test = which(fold == v)
# KNN (with an internal CV for picking K)
X2 = X[learn,]; Y2 = Y[learn]
K = 30; n2 = nrow(X2)
fold2 = rep(1:V, n2/V)
err.knn2 = matrix(NA, K, V)
for (v2 in 1:v){ # Start of the internal CV
learn2 = which(fold2 != v2)
test2 = which(fold2 == v2)
for (k in 1:K){
ystar = knn(X2[learn2,], X2[test2,], Y2[learn2], k)
err.knn2[k, v2] = sum(ystar != Y2[test2]) / length(ystar)
}
}
Kstar = which.min(rowMeans(err.knn2)) # Choice of K* for this fold
# Evaluation of the error for this fold
ystar = knn(X[learn,], X[test, ], y[learn], k=Kstar)
err.knn[v] = sum(ystar != Y[test]) / length(ystar)
# LDA
f <- lda(X[learn,], Y[learn])
ystar = predict(f, X[test,])$class
err.lda[v] = sum(ystar != Y[test]) /length(ystar)
}
data(iris)
X = iris[,-5]
Y = as.numeric(iris$Species)
xstar = c(6, 2.9, 5, 3)
f, svm = svm(X, Y, kernel = 'radial', gamma = 1, type = 'C-classification')
data(iris)
X = iris[,-5]
Y = as.numeric(iris$Species)
xstar = c(6, 2.9, 5, 3)
f.svm = svm(X, Y, kernel = 'radial', gamma = 1, type = 'C-classification')
install.package('e1071')
install.packages('e1071')
library(e1071)
data(iris)
X = iris[,-5]
Y = as.numeric(iris$Species)
xstar = c(6, 2.9, 5, 3)
f.svm = svm(X, Y, kernel = 'radial', gamma = 1, type = 'C-classification')
ystar = predict(f.svm, matrix(xstar, nrow=1))
ystar
rep(10, 10)
rep(10, 2)
V = 25 ; GAMMA = seq(0.01, 1, by=0.01) # GAMMA is always smaller than 1
n = nrow(X)
fold = rep(1:V, n/V)
err.svm = matrix(NA, K, V)
for (v in 1:V){
learn = which(fold !=v)
test = which(fold == v)
for (j in 1:length(GAMMA)){
gamma = GAMMA[j]
f.svm = svm(X[learn,], Y[learn], kernel = 'radial', gamma-gamma,
type = "C-classification")
ystar = predict(f.svm, X[test,])
err.svm[j,v] = sum(ystar != Y[test]) / length(ystar)  }
}
V = 25 ; GAMMA = seq(0.01, 1, by=0.01) # GAMMA is always smaller than 1
n = nrow(X)
fold = rep(1:V, n/V)
err.svm = matrix(NA, K, V)
for (v in 1:V){
learn = which(fold !=v)
test = which(fold == v)
for (j in 1:length(GAMMA)){
gamma = GAMMA[j]
f.svm = svm(X[learn,], Y[learn], kernel = 'radial', gamma-gamma,
type = "C-classification")
ystar = predict(f.svm, X[test,])
err.svm[j,v] = sum(ystar != Y[test]) / length(ystar)  }
}
V = 25 ; GAMMA = seq(0.01, 1, by=0.01) # GAMMA is always smaller than 1
n = nrow(X)
fold = rep(1:V, n/V)
err.svm = matrix(NA, length(GAMMA), V)
for (v in 1:V){
learn = which(fold !=v)
test = which(fold == v)
for (j in 1:length(GAMMA)){
gamma = GAMMA[j]
f.svm = svm(X[learn,], Y[learn], kernel = 'radial', gamma-gamma,
type = "C-classification")
ystar = predict(f.svm, X[test,])
err.svm[j,v] = sum(ystar != Y[test]) / length(ystar)
}
}
plot(GAMMA, rowMeans(err.svm), type='b')
V = 25 ; GAMMA = seq(0.01, 1, by=0.01) # GAMMA is always smaller than 1
n = nrow(X)
fold = rep(1:V, n/V)
err.svm = matrix(NA, length(GAMMA), V)
for (v in 1:V){
learn = which(fold != v)
test = which(fold == v)
for (j in 1:length(GAMMA)){
gamma = GAMMA[j]
f.svm = svm(X[learn,], Y[learn], kernel = 'radial', gamma-gamma,
type = "C-classification")
ystar = predict(f.svm, X[test,])
err.svm[j,v] = sum(ystar != Y[test]) / length(ystar)
}
}
plot(GAMMA, rowMeans(err.svm), type='b')
V = 25 ; GAMMA = seq(0.01, 1, by=0.01) # GAMMA is always smaller than 1
n = nrow(X)
fold = rep(1:V, n/V)
err.svm = matrix(NA, length(GAMMA), V)
for (v in 1:V){
learn = which(fold != v)
test = which(fold == v)
for (j in 1:length(GAMMA)){
gamma = GAMMA[j]
f.svm = svm(X[learn,], Y[learn], kernel = 'radial', gamma-gamma,
type = "C-classification")
ystar = predict(f.svm, X[test,])
err.svm[j,v] = sum(ystar != Y[test]) / length(ystar)
}
}
plot(GAMMA, rowMeans(err.svm), type='b')
which(c(1,2,3,4,5,6,7,8,9,10))
data(iris)
X = iris[,-5]
Y = as.numeric(iris$Species)
pca = princomp(X)
bplot(pca)
data(iris)
X = iris[,-5]
Y = as.numeric(iris$Species)
pca = princomp(X)
biplot(pca)
data("AirPassengers")
str(AirPassengers)
varicelle<-ts(data$x, start=c(1931, 1), end=c(1972, 6), freq=12)
plot(varicelle)
data = read.csv()
#install.packages("depmixS4")
library("depmixS4")
#install.packages("quantmod")
library("quantmod")
setwd("/Users/emisohpi/Desktop/DataFluct/アズビル")
df <- read.csv("cleaned.csv")
date <- as.character(df[,1])
date_TS<- as.POSIXlt(date) # Create date and time objects
TSData <- data.frame(df[,2], row.names=date_TS)
TSData <- as.xts(TSData) # Build our time series dataset
colnames(TSData) <- "pulse"
HMM <- depmix(list(pulse~1), data=TSData, nstates=20, family=list(poisson()))
data(swiss)
?swiss
library(class)
?hclust
library(class)
?hclust
D = dist(swiss)
hc = hclust(D, method="complete")
plot(hc)
library(class)
?hclust
D = dist(swiss)
hc = hclust(D, method="complete")
plot(hc)
82)
library(class)
?hclust
D = dist(swiss)
hc = hclust(D, method="complete")
plot(hc)
library(tidyverse)
library(cluter)
install.packages("cluter")
install.packages("factoextra")
install.packages("gridExtra")
install.packages("factoextra")
swiss
kmeans2 <- kmeans(swiss)
kmeans2 <- kmeans(as.matrix(swiss))
as.matrix(swiss)
kmeans2 <- kmeans(as.matrix(swiss), centers=2)
kmeans
str(kmeans2)
fviz_cluster(kmeans2, data=swiss)
library(gridExtra)
kmeans2 <- kmeans(as.matrix(swiss), centers=2)
fviz_cluster(kmeans2, data=swiss)
library(gridExtra)
fviz_cluster(kmeans2, data=swiss)
library(class)
?hclust
D = dist(swiss)
par(mfrow=c(2,2))
hc1 = hclust(D, method="complete"); plot(hc1)
hc2 = hclust(D, method="single"); plot(hc2)
hc3 = hclust(D, method="centroid"); plot(hc3)
hc4 = hclust(D, method="word.D2");plot(hc4)
library(class)
?hclust
D = dist(swiss)
par(mfrow=c(2,2))
hc1 = hclust(D, method="complete"); plot(hc1)
hc2 = hclust(D, method="single"); plot(hc2)
hc3 = hclust(D, method="centroid"); plot(hc3)
hc4 = hclust(D, method="ward.D2");plot(hc4)
hc4=hclust(D,method="ward.D2")
cl4=cutree(hc4, k=2)
cl4
plot(cl4)
hc4=hclust(D,method="ward.D2")
cl4=cutree(hc4, k=2)
plot(cl4, colnames(hc4))
help(cl4)
??cl4
?cl4
??cutree
hc4=hclust(D,method="ward.D2")
cl4=cutree(hc4, k=2)
cutree_1h.dendrogram(hc4, h=50)
hc4=hclust(D,method="ward.D2")
cl4=cutree(hc4, k=2)
cl4
hc4=hclust(D,method="ward.D2")
cl4=cutree(hc4, k=2)
dennd=as.dendrogram(hc4)
hc4=hclust(D,method="ward.D2")
cl4=cutree(hc4, k=2)
dend=as.dendrogram(hc4)
cutree_1h.dendrogram(dend, h=50)
hc4=hclust(D,method="ward.D2")
cl4=cutree(hc4, k=2)
cl4
plot(swiss, col=cl4, pch=19)
library(class)
?hclust
D = dist(swiss)
par(mfrow=c(2,2))
hc1 = hclust(D, method="complete"); plot(hc1)
hc2 = hclust(D, method="single"); plot(hc2)
hc3 = hclust(D, method="centroid"); plot(hc3)
hc4 = hclust(D, method="ward.D2");plot(hc4)
hc4=hclust(D,method="ward.D2")
cl4=cutree(hc4, k=2)
cl4
install.packages("cluter")
install.packages("factoextra")
install.packages("gridExtra")
library(tidyverse)
library(cluter)
install.packages("gridExtra")
#install.packages("cluter")
#install.packages("factoextra")
#install.packages("gridExtra")
library(tidyverse)
library(cluter)
install.packages("cluter")
install.packages("factoextra")
install.packages("gridExtra")
library(tidyverse)
library(cluter)
install.packages("gridExtra")
#install.packages("cluter")
#install.packages("factoextra")
#install.packages("gridExtra")
library(tidyverse)
library(cluter)
install.packages("cluter")
#install.packages("factoextra")
#install.packages("gridExtra")
library(tidyverse)
library(cluter)
