---
title: "R Notebook"
output: html_notebook
---


# Learning in high dimensions
## Mixture of PPCA and HDDC

The two methods are available in th eHDclassif package:

```{r}
#install.packages("HDclassif")
library(HDclassif)
```

The packages contains two main functions:

- `hddc` which allows to cluster high-dimensional data
- `hdda` for the supervised classification of HD data

Let's first focus on the supervised classification problem
LDA and SVM for instance. A basic call of hdda is as follows:

```{r}
install.packages("MBCbook")
library(MBCbook)
```

```{r}
data("usps358")
```

These data are vectorized version of handwritten digits provided by the U.S.Service.
It is possible to visualize the 16x16 images as follows:

```{r}
dim(usps358)
X = usps358[,-1]
cls=usps358$cls

# Visualize some of digits:
#par(mfrow=c(1,4))
imshow(matrix(t(X[134,]), nrow=16, byrow=TRUE))
```

```{r}
learn = sample(1:nrow(X), 1500)
f = hdda(X[learn,], cls[learn], model="AkjBkQkDk")
Yhat = predict(f, X[-learn,])
err.hdda = sum(Yhat$class != cls[-learn])/length(Yhat$class)
err.hdda
```

Here, because we have a fitted GMM, we can have a look at the model parameters:

```{r}
# The class-specific dimensionalities
f$d

# The variance of 
#f$mu

#f$a

#f$b
```

It is of course possible to look at the means as average digits:

```{r}
par(mfrow=c(1,3))
imshow(matrix(t(f$mu[1,]), nrow=16, byrow=TRUE))
imshow(matrix(t(f$mu[2,]), nrow=16, byrow=TRUE))
imshow(matrix(t(f$mu[3,]), nrow=16, byrow=TRUE))
```

> Exercise: look at the effect of varying the threshold $\tau$ on the classification error

```{r}
tau = seq(200, 1, by= -10)/1000
err = matrix(NA, 10, 20)
for (i in 1:10){cat('.')
  learn = sample(1:nrow(X), 1500)
  for(j in 1:length(tau)){
    f = hdda(X[learn,], cls[learn], threshold = tau[j])
    Yhat = predict(f, X[-learn,])
    err[i,j] = sum(Yhat$class != cls[-learn])/length(Yhat$class)
  }
}
plot(tau,colMeans(err), type~"b")
```

```{r}
learn = sample(1:nrow(X), 1500)
f = hdda(X[learn,], cls[learn], d_select="CV",
         cv.threshold = c(0.2, 0.1, 0.05, 0.01, 0.005, 0.001))
Yhat = predict(f, X[-learn,])
err.hdda = sum(Yhat$class != cls[-learn])/length(Yhat$class)
err.hdda
```

```{r}
err = matrix(NA, 2,25)
for(i in 1:25){
  learn = sample(1:nrow(X), 1500)
  f = hdda(X[learn,], cls[learn], d_select="CV",
           cv.threshold=c(0.2,0.1,0.05,0.01,0.005,0.001))
  Yhat = predict(f, X[-learn,])
  err[1,i] = sum(Yhat&class != cls[-learn]) / length(Yhat$class)
  
  g = lda(X[learn,], cls[learn])
  Yhat = predict(g, X[-learn,])
  err[2,i] = sum(Yhat$class != cls[-learn]) / length(Yhat$class)
}
boxplot(err, names=c("HDDA", "LDA"), col=c("lavender", "pink"))

```

Let's now move to the "clustering proble":
```{r}
data("usps358")
X = usps358[,-1]
cls = usps358$cls

out = hddc(X, k=3)

table(cls, out$class)
```
